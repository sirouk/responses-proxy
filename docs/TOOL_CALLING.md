# Tool Calling Implementation

Full implementation of OpenAI Responses API tool calling (function calling) with proper event streaming.

## Features

- ‚úÖ **Function definitions** - Define tools with JSON schema parameters
- ‚úÖ **Tool choice control** - `auto`, `none`, `required`, or force specific tool
- ‚úÖ **Parallel tool calls** - Multiple simultaneous function calls
- ‚úÖ **Streaming events** - Proper delta and done events for function arguments
- ‚úÖ **Multi-turn support** - Send function results back for continued conversation
- ‚úÖ **Type safety** - Full Rust type system for tool call states
- ‚úÖ **Resilient parsing** - Converts stray XML-style tool call text into real function calls

## Supported Parameters

### Request Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `tools` | array | Array of tool definitions (currently supports `type: "function"`) |
| `tool_choice` | string or object | How model selects tools |
| `parallel_tool_calls` | boolean | Enable parallel function execution (default: true) |

### Tool Choice Values

```json
// Let model decide (default)
"tool_choice": "auto"

// Disable all tools
"tool_choice": "none"

// Require at least one tool call
"tool_choice": "required"

// Force specific tool
"tool_choice": {
  "type": "function",
  "function": {"name": "get_weather"}
}
```

## Tool Definition Format

```json
{
  "type": "function",
  "function": {
    "name": "get_weather",
    "description": "Get the current weather in a given location",
    "parameters": {
      "type": "object",
      "properties": {
        "location": {
          "type": "string",
          "description": "City and state"
        },
        "unit": {
          "type": "string",
          "enum": ["celsius", "fahrenheit"]
        }
      },
      "required": ["location"]
    }
  }
}
```

## XML-Style Compatibility Layer

Some upstream providers still emit ad-hoc XML wrappers for tool calls (for example `<function=apply_patch>` blocks). The proxy detects these fragments in assistant deltas, strips them from the visible text, and emits proper `function_call` items with JSON arguments. This conversion is guarded so that:

- Plain text responses remain untouched when no XML markers are present.
- Multiple tool calls in a single block are converted sequentially with deterministic IDs.
- The cleaned assistant text continues streaming to the client without exposing the raw XML.

This behaviour is automatic‚Äîno additional configuration required.

## Event Sequence

When the model calls a function, you'll receive these events:

### 1. Output Item Added
```json
{
  "type": "response.output_item.added",
  "item_id": "call_abc123",
  "output_index": 1,
  "item": {
    "id": "call_abc123",
    "type": "function_call",
    "status": "in_progress",
    "call_id": "call_xyz",
    "name": "get_weather",
    "arguments": ""
  }
}
```

### 2. Tool Call Begin
```json
{
  "type": "response.output_tool_call.begin",
  "item_id": "call_abc123",
  "output_index": 1,
  "call_id": "call_xyz",
  "name": "get_weather"
}
```

### 3. Arguments Delta (streaming)
```json
{
  "type": "response.output_tool_call.delta",
  "item_id": "call_abc123",
  "output_index": 1,
  "delta": "{\"location\":"
}
```

> Legacy `response.function_call_arguments.delta` events are still emitted for compatibility, but `response.output_tool_call.delta` is now the canonical stream.

### 4. Arguments Done
```json
{
  "type": "response.output_tool_call.end",
  "item_id": "call_abc123",
  "output_index": 1,
  "name": "get_weather",
  "arguments": "{\"location\":\"San Francisco, CA\"}"
}
```

> A matching `response.function_call_arguments.done` event is also emitted for older clients.

### 5. Output Item Done
```json
{
  "type": "response.output_item.done",
  "item_id": "call_abc123",
  "output_index": 1,
  "item": {
    "id": "call_abc123",
    "type": "function_call",
    "status": "completed",
    "call_id": "call_xyz",
    "name": "get_weather",
    "arguments": "{\"location\":\"San Francisco, CA\"}"
  }
}
```

### 6. Response Completed
```json
{
  "type": "response.completed",
  "response": {
    "id": "resp_123",
    "status": "completed",
    "output": [
      {
        "type": "message",
        "role": "assistant",
        "content": [...]
      },
      {
        "type": "function_call",
        "call_id": "call_xyz",
        "name": "get_weather",
        "arguments": "{\"location\":\"San Francisco, CA\"}"
      }
    ]
  }
}
```

## Multi-Turn Conversations

To continue a conversation after a tool call:

1. **Extract function call from response** - Get `call_id`, `name`, and `arguments`
2. **Execute the function** - Run your actual function code
3. **Send result back** - Include previous messages plus function result

```json
{
  "model": "gpt-4o-mini",
  "input": [
    {"type": "message", "role": "user", "content": "What's the weather?"},
    {"type": "message", "role": "assistant", "content": ""},
    {
      "type": "function_call",
      "call_id": "call_xyz",
      "name": "get_weather",
      "arguments": "{\"location\":\"San Francisco\"}"
    },
    {
      "type": "function_call_output",
      "call_id": "call_xyz",
      "output": "{\"temp\":72,\"condition\":\"sunny\"}"
    }
  ],
  "tools": [...],
  "stream": true
}
```

## Parallel Tool Calls

When `parallel_tool_calls: true` (default), the model can call multiple functions:

```json
{
  "output": [
    {"type": "message", ...},
    {
      "type": "function_call",
      "name": "get_weather",
      "arguments": "{\"location\":\"Boston\"}"
    },
    {
      "type": "function_call", 
      "name": "get_weather",
      "arguments": "{\"location\":\"Seattle\"}"
    }
  ]
}
```

Each tool call has:
- Unique `call_id`
- Unique `output_index` (1, 2, 3, ...)
- Separate event streams

## Output Item Types

### Message (index 0)
```json
{
  "type": "message",
  "role": "assistant",
  "content": [{"type": "output_text", "text": "..."}]
}
```

### Function Call (index 1+)
```json
{
  "type": "function_call",
  "call_id": "call_xyz",
  "name": "get_weather",
  "arguments": "{...}"
}
```

## Implementation Details

### Backend Translation

The proxy translates between formats:

**Responses API ‚Üí Chat Completions:**
- `tools` array ‚Üí `tools` array (function type only)
- `tool_choice` ‚Üí `tool_choice`
- `parallel_tool_calls` ‚Üí `parallel_tool_calls`
- `function_call` input ‚Üí assistant message with `tool_calls`
- `function_call_output` input ‚Üí `tool` role message

**Chat Completions ‚Üí Responses API:**
- Delta `tool_calls` ‚Üí `output_tool_call.delta` (plus legacy `function_call_arguments.delta`)
- Tool call starts ‚Üí `output_tool_call.begin` + `output_item.added`
- Tool call completion ‚Üí `output_tool_call.end` + `function_call` output items
- Maintains proper `output_index` ordering

### State Tracking

For each tool call, the proxy tracks:
- `id` - Call identifier
- `type` - Always "function" currently
- `name` - Function name
- `arguments` - Accumulated JSON arguments
- `item_added` - Whether output_item.added was sent
- `end_emitted` - Whether end events were sent (prevents duplicates)
- `pending_args` - Arguments buffered before name arrives

**Fragmentation Handling**: If the backend sends tool arguments before the function name (valid OpenAI behavior), the proxy buffers them in `pending_args` and replays them immediately after emitting the `begin` event. This ensures event ordering is always: `begin` ‚Üí `delta` ‚Üí `end`.

### Event Sequencing

Events are numbered sequentially:
1. `response.created`
2. `response.output_item.added` (message)
3. `response.content_part.added` (message text)
4. Multiple `response.output_text.delta` (text chunks)
5. `response.output_tool_call.begin` + `response.output_item.added` (function call)
6. Multiple `response.output_tool_call.delta` *(legacy `response.function_call_arguments.delta` still emitted)*
7. `response.output_tool_call.end` *(legacy `response.function_call_arguments.done` still emitted)*
8. `response.output_item.done` (function call)
9. `response.output_text.done`
10. `response.output_item.done` (message)
11. `response.completed`
12. `response.done` (terminal event)

## Examples

See:
- `tests/manual/tool_calling_simple.py` - Python streaming example
- `tests/manual/tool_calling_example.sh` - Multi-turn bash example
- `test_tool_calling.sh` - Test suite with multiple scenarios

## Limitations

**Currently supported:**
- Function tools only (not file_search, web_search, code_interpreter, etc.)
- Stateless operation (no conversation persistence)
- Tool calls from model to client

**Not yet supported:**
- Built-in tools (file_search, web_search, etc.) - filtered out
- Custom tool types
- Tool call cancellation
- Background responses with tools

## Testing

Run tool calling tests:
```bash
./test_tool_calling.sh
```

Run specific test:
```bash
python tests/manual/tool_calling_simple.py
```

## Debugging

Enable debug logging to see tool call processing:
```bash
RUST_LOG=debug cargo run
```

Look for log messages:
- `üîß Tool call started: <name> (index <N>)`
- `üîç Buffering <N> argument bytes for tool index <I> (name not yet received)` - fragmentation detected
- `üîß Replaying <N> buffered argument bytes for <name>` - buffered args replayed
- `üîß Tool call complete: <name> - <bytes> bytes of args`
- `üîß INPUT: Found function_call (<name>) - will attach to assistant message`
- `üîß INPUT: Added function_call_output (call_id: <id>)`

## Architecture

```
Client Request (Responses API with tools)
         ‚Üì
   Request Parser
         ‚Üì
   Tool Definitions ‚Üí Chat Completions format
         ‚Üì
   Backend (Chutes.ai)
         ‚Üì
   Streaming Response Parser
         ‚Üì
   Tool Call State Tracker (HashMap with buffering)
         ‚Üì
   Event Generator:
   - output_tool_call.begin + output_item.added (function_call)
   - output_tool_call.delta + function_call_arguments.delta
   - output_tool_call.end + function_call_arguments.done
   - output_item.done (function_call)
         ‚Üì
   SSE Stream to Client
```

**Fragmentation Safety**: The state tracker buffers early argument chunks in `pending_args` until the function name arrives, then replays them in correct order.

## Performance

- **Overhead:** 1-2ms for tool call event generation
- **Memory:** ~250 bytes per tool call state (includes pending_args buffer)
- **Streaming:** Minimal buffering (only for fragmented tool headers)
- **Parallel calls:** Handled efficiently with HashMap tracking
- **Fragmentation:** Buffering overhead negligible (<1Œºs per chunk)

## Error Handling

If backend returns malformed tool calls:
- Invalid JSON in arguments ‚Üí logged but passed through
- Missing fields ‚Üí defaults applied
- Tool call errors ‚Üí logged, response continues

Circuit breaker applies to tool calling requests same as regular requests.


